{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import sys\n",
    "input = \"Hotel_Reviews.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(input, sep = ',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "387848\n",
      "479792\n",
      "290886\n",
      "96962\n",
      "290886\n",
      "96962\n"
     ]
    }
   ],
   "source": [
    "neg_revs = []\n",
    "pos_revs = []\n",
    "for index, row in df.iterrows():\n",
    "    if row['Negative_Review'] != 'No Negative':\n",
    "        neg_revs.append(row['Negative_Review'])\n",
    "    if row['Positive_Review'] != 'No Positive':\n",
    "        pos_revs.append(row['Positive_Review'])\n",
    "print(len(neg_revs))\n",
    "print(len(pos_revs))\n",
    "train_pos = pos_revs[0:int(0.75*len(neg_revs))]\n",
    "test_pos = pos_revs[int(0.75*len(neg_revs)):len(neg_revs)]\n",
    "train_neg = neg_revs[0:int(0.75*len(neg_revs))]\n",
    "test_neg = neg_revs[int(0.75*len(neg_revs)):]\n",
    "print(len(train_pos))\n",
    "print(len(test_pos))\n",
    "print(len(train_neg))\n",
    "print(len(test_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomine/.anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3334: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "stats_neg_words = {}\n",
    "\n",
    "for doc in train_neg:\n",
    "    t = set([tok for tok in word_tokenize(doc)]) ## now a word occurs only once \n",
    "    for w in t:\n",
    "        if w in stats_neg_words:\n",
    "            stats_neg_words[w] += 1\n",
    "        else:\n",
    "            stats_neg_words[w] = 1\n",
    "\n",
    "## we divide by the number of negative docs in order to have values between 0 and 1            \n",
    "for w in stats_neg_words:\n",
    "    stats_neg_words[w] = stats_neg_words[w]/len(train_neg) \n",
    "\n",
    "## same work for positive documents\n",
    "stats_pos_words = {}\n",
    "\n",
    "for doc in train_pos:\n",
    "    t = set([tok for tok in word_tokenize(doc)])\n",
    "    for w in t:\n",
    "        if w in stats_pos_words:\n",
    "            stats_pos_words[w] += 1\n",
    "        else:\n",
    "            stats_pos_words[w] = 1\n",
    "\n",
    "for w in stats_pos_words:\n",
    "    stats_pos_words[w] = stats_pos_words[w]/len(train_pos) \n",
    "    \n",
    "\n",
    "    \n",
    "## we complete stats_pos_words by adding each\n",
    "## word that are in stats_neg_words and not in stats_pos_words\n",
    "## but with score 0\n",
    "for w in stats_neg_words:\n",
    "  if not w in stats_pos_words:\n",
    "    stats_pos_words[w] = 0\n",
    "\n",
    "## same work for stats_neg_words\n",
    "for w in stats_pos_words:\n",
    "  if not w in stats_neg_words:\n",
    "    stats_neg_words[w] = 0\n",
    "    \n",
    "with open('positive.pkl', 'wb') as f:\n",
    "    pickle.dump(stats_pos_words, f)  \n",
    "with open('negative.pkl', 'wb') as f:\n",
    "    pickle.dump(stats_neg_words, f)\n",
    "sys.exit()    \n",
    "## examples    \n",
    "print(stats_pos_words[\"bad\"])\n",
    "print(stats_neg_words[\"bad\"])\n",
    "print(stats_pos_words[\"good\"])\n",
    "print(stats_neg_words[\"good\"])\n",
    "\n",
    "## P(w|?) ? can be positive or negative if stats is positive dictionary or negative dictionary\n",
    "def score_word(w,stats):\n",
    "    if w not in stats:\n",
    "        return 0\n",
    "    return stats[w]\n",
    "\n",
    "## positivity of the word\n",
    "def score(w,stats_p,stats_n):\n",
    "    ## ATTENTION : I must take into account that, during test conditions,\n",
    "    ## w can be not in the training positive docs and not int the training negative docs\n",
    "    sp = score_word(w,stats_p)\n",
    "    sn = score_word(w,stats_n)\n",
    "    if sp+sn == 0:\n",
    "        ## then, by default return 0 : not negative, not positive\n",
    "        return 0\n",
    "    return (sp-sn)/(sp+sn)\n",
    "\n",
    "\n",
    "## positivity of a document\n",
    "def eval(doc,stats_p,stats_n,threshold):\n",
    "    t = set([tok for tok in word_tokenize(doc)])\n",
    "    s = 0\n",
    "    ## sum for all word in the document\n",
    "    for w in t:\n",
    "        s += score(w,stats_p,stats_n)\n",
    "    ## return decision\n",
    "    if s > threshold:\n",
    "        return \"P\"\n",
    "    else:\n",
    "        return \"N\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== 0\n",
      "recall 0.8279222788308822\n",
      "precision 0.9472883035967148\n",
      "F1 0.8835921763728221\n",
      "================== 1\n",
      "recall 0.6613931230791444\n",
      "precision 0.9844193721697752\n",
      "F1 0.7912058085091946\n",
      "================== 2\n",
      "recall 0.4673274066128999\n",
      "precision 0.9951901959061759\n",
      "F1 0.63599870871756\n",
      "================== 3\n",
      "recall 0.31483467750252675\n",
      "precision 0.9972233111198223\n",
      "F1 0.47857713954253994\n",
      "================== 4\n",
      "recall 0.19394195664280853\n",
      "precision 0.9983012156925201\n",
      "F1 0.32478691525833553\n",
      "================== 5\n",
      "recall 0.11353932468389678\n",
      "precision 0.9982771128037722\n",
      "F1 0.20388924900453745\n",
      "================== 6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-a15c94259aa8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_neg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstats_pos_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstats_neg_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"P\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mfp\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-ee58dd71f4ff>\u001b[0m in \u001b[0;36meval\u001b[0;34m(doc, stats_p, stats_n, threshold)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m## positivity of a document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstats_p\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstats_n\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtok\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m## sum for all word in the document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     return [\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     ]\n",
      "\u001b[0;32m~/.anaconda3/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     return [\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     ]\n",
      "\u001b[0;32m~/.anaconda3/lib/python3.7/site-packages/nltk/tokenize/treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONTRACTIONS2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr' \\1 \\2 '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONTRACTIONS3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr' \\1 \\2 '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for t in [0,1,2,3,4,5,6,7,8,9,10]:\n",
    "    print(\"==================\",t)\n",
    "\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "\n",
    "    for doc in test_neg:\n",
    "        if eval(doc,stats_pos_words,stats_neg_words,t) == \"P\":\n",
    "            fp += 1\n",
    "        else:\n",
    "            tn += 1\n",
    "\n",
    "    for doc in test_pos:\n",
    "        if eval(doc,stats_pos_words,stats_neg_words,t) == \"P\":\n",
    "            tp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "\n",
    "    recall = tp/(tp+fn)\n",
    "    precision = tp/(tp+fp)\n",
    "    if precision and recall:\n",
    "        F1 = 2*(precision*recall)/(precision+recall)\n",
    "\n",
    "        print(\"recall\",recall)\n",
    "        print(\"precision\",precision)\n",
    "        print(\"F1\",F1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
